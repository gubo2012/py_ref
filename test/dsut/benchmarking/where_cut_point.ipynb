{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting a Threshold for Predictive Models\n",
    "\n",
    "By [*Andrew Wheeler, PhD*](mailto:andrew.wheeler@hms.com)\n",
    "\n",
    "Most predictive models we either get a predicted probability, $\\hat{p}(x)$, or a continuous valued prediction, e.g. $\\mathbb{E}[x]$ (ignoring the variance part of either prediction for now). This prediction though does not directly translate into an action you should take.\n",
    "\n",
    "There are generally two things you need to take into account when determining *what to do* with the prediction from the model. One is the costs & benefits associated with taking a particular action (the costs when you are wrong, and the benefits when you are right). The second is constraints on what you can do with the information (e.g. human auditors can only review so many cases).\n",
    "\n",
    "This example just focuses on costs/benefits, I will create a notebook with examples of dealing with constraints in the future.\n",
    "\n",
    "## Cost Simple Example\n",
    "\n",
    "Say you get a predicted probability of an event occurring that is 5%, does that mean you should do nothing, or take some action? It depends; imagine a scenario where the 5% is the probability that it will rain today, and the decision is whether to take your umbrella with you to work. \n",
    "\n",
    "Now say that you are buying a used car, and 5% is the probability it will not run when you buy it. \n",
    "\n",
    "For the former carrying umbrella decision, you probably would not worry about carrying it. For the later car buying decision, 5% is a little too high for my tastes for such a large investment. \n",
    "\n",
    "The difference between these two examples are the *costs* associated with each. If you forget to carry your umbrella, it is annoying, but not that big of a deal. If you sink several thousand dollars into a car that does not work, that is much worse. \n",
    "\n",
    "In general, we need to figure out the costs with making a bad decision and the benefits of making a good decision to be able to reason about what action we should take given a 5% probability of an outcome occurring. This is as true for individual predictions in our daily lives, as it is for evaluating millions of transactions and deciding which ones should be audited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathy Part 1: Weighing True Positives and False Positives\n",
    "\n",
    "So many of our business models are predicting a binary outcome, like fraud/soft-denial, etc. In these cases, we can break down the four outcomes into the following:\n",
    "\n",
    " - Take Action, Prediction Right (True Positive)\n",
    " - Take Action, Prediction Wrong (False Positive)\n",
    " - Do Not Take Action, Prediction Right (True Negative)\n",
    " - Do Not Take Action, Prediction Wrong (False Negative)\n",
    "\n",
    "We can subsequently assign costs and benefits to each of these cases. Costs are often only associated with the \"Take Action\" events, doing nothing often has no costs nor benefits. (Although sometimes you can say the false negatives are potentially leaving money on the table.) \n",
    "\n",
    "For a concrete example, we can flag a claim to be reviewed by an auditor. It costs the auditors time no matter what, but if we identify a true positive, we gain additional value. \n",
    "\n",
    "Besides identifying those cost/benefits, we need one additional piece of information when determining where to set the threshold, the overall prevalence of the outcome in the population under study. I will give an example below why that is the case, but for now note that for very rare events, you will have many more false positives than true positives, even if the model is very good. \n",
    "\n",
    "So lets give a specific example. Say we generated predictions for 1,000 claims, and are attempting to identify fraud. We then have three pieces of information:\n",
    "\n",
    " 1) We think the overall prevalence of fraud in our sample is around 10%\n",
    " 2) The Sensitivity of our predictive instrument (the proportion of true cases we capture), is 90%\n",
    " 3) The False Positive Rate of our predictive instrument is 5%\n",
    "\n",
    "So given our 1000 cases, we can break them down into the following categories:\n",
    "\n",
    " - 1,000 Total Cases\n",
    "   - 100 Positive Cases (10% of Total Cases)\n",
    "     - 90 True Positives (90% of Positive Cases)\n",
    "     - 10 False Negatives\n",
    "   - 900 Negative Cases\n",
    "     - 855 True Negatives\n",
    "     -  45 False Positives (5% of Negative Cases)\n",
    "     \n",
    "So using simple example lets also say the *cost* of assigning a case to an auditor is \\\\$2, and the benefit of identifying a positive case is \\\\$10 (so a net of \\\\$8). In this example, using our predictive instrument, we then have (where `TP` is true positives, and `FP` is false positives.\n",
    "\n",
    "`Utility = (10-2)*TP + -2*FP = 8*90 + -90 = 630`\n",
    "\n",
    "So we have netted $630 in this example. We pretty much always have the capabilities to change our internal thresholds. So we could lower the bar as to what is flagged, which will increase the sensitivity, but also increase the false positive rate. You can then subsequently graph this relationship given different thresholds\n",
    "\n",
    "`Utility = N*Prev*Sen*B + N*(1-Prev)*FPR*C`\n",
    "\n",
    "Where the variables are:\n",
    "\n",
    "  - `Benefit` is the total benefit of using a particular threshold\n",
    "  - `N` is the total number of cases we are evaluating\n",
    "  - `Prev` is the prevalence of the outcome\n",
    "  - `Sen` is the sensitivity of the test (e.g. captures 90/100 true positives)\n",
    "  - `FPR` is the false positive rate of the test (e.g. if you do the test on 100 true negatives, you get 5 false positives)\n",
    "  - `B` is the benefit of identifying a true positive, and `C` is the cost of a false positive\n",
    "  \n",
    "So we can factor out the N and place it on the left hand side, making it a per-capita utility measure.\n",
    "\n",
    "`Utility/N = Prev*Sen*B + (1 - Prev)*FPR*C`\n",
    "\n",
    "Prevalence is a fixed sample property. The Sensitivity and False-Positive Rate are often not simple functions (although can be estimated with a hold out sample). So you typically need to use a graph (or grid search) to find the best solution. \n",
    "\n",
    "It can be the case that different cases have different benefits. Say a cases benefits are related to the total amount of the claims. In that case you may have different thresholds. \n",
    "\n",
    "## Example Use Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "#Libraries used and my defined functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#Now importing my own functions I made\n",
    "import sys\n",
    "import os\n",
    "locDir = r'C:\\Users\\e009156\\Documents\\DataScience_Notes\\WhereToSet_Threshold' #Is there a better way to make this relative?\n",
    "sys.path.append(locDir)\n",
    "from cut_point_functions import *\n",
    "\n",
    "#I don't save anything in this example, so no need to change the directory\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds, utility curve, and optimal cut point\n",
      "[1.9  0.9  0.8  0.4  0.3  0.05]\n",
      "[ 0.    0.2   0.4  -0.05  0.35 -1.  ]\n",
      "0.8\n",
      "\n",
      "Cut Point with higher weight for capturing true positives\n",
      "0.3\n",
      "\n",
      "Confusion Matrix and estimated utility\n",
      "4 0 2 2\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#This first part is a simple example \n",
    "#of calculating the cost/benefit\n",
    "#of varying cut-points\n",
    "###########################################################\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "scores = np.array([0.1, 0.2, 0.05, 0.4, 0.9, 0.3, 0.35, 0.8])\n",
    "\n",
    "#ROC curve metrics: TPR, FPR, and thresholds, on TESTING data\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=1)\n",
    "\n",
    "##################\n",
    "#Assign benefits/costs & prevalance and graph that function\n",
    "co = -2\n",
    "be = 10 + co\n",
    "prev = 0.1  #could also be np.mean(y) to match data here\n",
    "#################\n",
    "\n",
    "#Estimating cut point using my function \n",
    "my_cut, my_curve = util_cut_point(tp=tpr,fp=fpr,th=thresholds,pr=prev,be=be,co=co,curve=True)   \n",
    "print(\"Thresholds, utility curve, and optimal cut point\")\n",
    "print(thresholds)\n",
    "print(my_curve)\n",
    "print(my_cut)\n",
    "    \n",
    "#Note the cut point will change if different cases have different utility\n",
    "#Eg maybe a case with a higher claim has more upside, even if prevalence of\n",
    "#fraud is the same\n",
    "\n",
    "print(\"\")\n",
    "print(\"Cut Point with higher weight for capturing true positives\")\n",
    "print(util_cut_point(tp=tpr,fp=fpr,th=thresholds,pr=prev,be=be+1000,co=co))    \n",
    "              \n",
    "\n",
    "#Estimating utility on hold out sample, \"my_cut\" can be a scaler or a numpy array\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y == 1, scores >= my_cut).ravel()\n",
    "ut = tp*be + fp*co\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix and estimated utility\")\n",
    "print(tn, fp, fn, tp) #confusion matrix\n",
    "print(ut) #total utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e009156\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#A LARGER DATA EXAMPLE \n",
    "#should do a dataset with a rare outcome to illustrate\n",
    "###########################################################\n",
    "\n",
    "###################################\n",
    "#Stealing example from here\n",
    "#https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Train and test\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=44)\n",
    "clf = LogisticRegression(penalty='l2', C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_prob = clf.predict_proba(X_test)[::,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence of Breast Cancer in the Training Data\n",
      "0.6220472440944882\n",
      "\n",
      "Cut Point based on orig data\n",
      "0.09384444559429438\n",
      "\n",
      "tn, fp, fn, tp\n",
      "62 6 0 120\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "#Getting ROC curve stats\n",
    "fp_canc, tp_canc, thresh_canc = metrics.roc_curve(y_test, y_pred_prob, pos_label=1)\n",
    "\n",
    "#Setting weights and estimating prevalence from the data\n",
    "cost = -1\n",
    "bene = 3 + cost\n",
    "prev = np.mean(y_train)\n",
    "print(\"Prevalence of Breast Cancer in the Training Data\")\n",
    "print(prev)\n",
    "\n",
    "bc_cut, bc_curve = util_cut_point(tp=tp_canc,fp=fp_canc,th=thresh_canc,pr=prev,be=bene,co=cost,curve=True)  \n",
    "print(\"\")\n",
    "print(\"Cut Point based on orig data\")\n",
    "print(bc_cut)\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test == 1, y_pred_prob >= bc_cut).ravel()\n",
    "ut = tp*bene + fp*cost\n",
    "print(\"\")\n",
    "print(\"tn, fp, fn, tp\")\n",
    "print(tn, fp, fn, tp) #confusion matrix\n",
    "print(ut) #total utility\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo \n",
    "\n",
    " - example sensitivity analysis if costs/benefits and/or prevalence have distributions instead of point estimates\n",
    "   - basically draw the utility curve over many hypotheticals, and you get a polygon of best cost/benefits\n",
    "   \n",
    " - using k-fold cross-validation on training data to estimate error in threshold estimate\n",
    "   - will still need to pipe in relevant costs/benefits/prevalence estimates\n",
    "\n",
    "## Other references for cutpoints\n",
    "\n",
    " - The `OptimalCutpoints` [R package](https://cran.r-project.org/web/packages/OptimalCutpoints/OptimalCutpoints.pdf) has many metrics\n",
    " - Hand article with adjusted AUC measure to incorporate weights and prevalence in population, *Measuring classifier performance: A coherent alternative to the area under the ROC curve*, ([Hand, 2009](http://www.cs.iastate.edu/~cs573x/Notes/hand-article.pdf))\n",
    " - My [blog example using relative weights](https://andrewpwheeler.com/2015/05/27/how-wide-to-make-the-net-in-actuarial-tools-false-positives-versus-false-negatives/) instead of absolute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
